# CS224自然语言处理中的数学推导

## 1. word2vec和GloVe

1. 模型任务：遍历文本中的所有位置，对于每一个位置上的窗口，中心词为c, 上下文单词为o, 通过计算两个向量之间的相似度，得出$p(o|c)$, 再通过调整参数(词向量组合)来的到最高的正确率。

2. 目标函数：
   $$
   L(\theta) = \prod_{t=1}^T\prod_{-m\leq j\leq m\;\\\;\;\;\ j\neq0} p(w_{t+j}|w_t;\theta)
   $$
   上面的其实是`Likelihood`函数，将上述函数作如下转换：
   $$
   J(\theta)=-\frac{1}{T}log\;L(\theta) = -\frac{1}{T}\sum_{t=1}^T\sum_{-m\leq j\leq m\;\\\;\;\;\ j\neq0}log\;p(w_{t+j}|w_t;\theta)
   $$
   即最小化此函数即可。其中$\frac{1}{T}$是作归一化处理，因为后面的值加起来最大为T

   计算$p(w_{t+j}|w_t;\theta)$的方法如下：
   $$
   p(o|c) = \frac{exp(u_o^Tv_c)}{\sum_{w\epsilon V}exp(u_w^Tv_c)}
   $$
   即通过点积计算相似度，相似度即度量两个单词同时出现的概率。

   在这里，$\theta$ 代表模型的所有参数，由于每个单词有两个词向量表示，所以最终数目为2dV，d为词向量的维度，V是词汇表的长度。

3. 梯度优化更新参数

   求[梯度](https://blog.csdn.net/xidianliutingting/article/details/51673207)：
   $$
   \frac{\partial}{\partial v_c}log\;\frac{exp(u_o^Tv_c)}{\sum_{w\epsilon V}exp(u_w^Tv_c)}\\
   = \frac{\partial}{\partial v_c}log\;exp(u_o^Tv_c) -  \frac{\partial}{\partial v_c}log\;\sum_{w=1}^{V}exp(u_w^Tv_c) \\
   = u_o - \sum_{x=1}^Vp(x|c)u_x
   $$
   这样做的话，需要语料库的所有位置，然后计算梯度之和，最后进行一次参数的更新，所以参数更新一次需要很长的时间。我们采用的方法是在每一个窗口都计算一次目标函数(Stochastic Gradient Descent, SGD)，然后计算梯度，以此来代替总体的梯度，这种方法十分的粗糙，每前进一步并不能保证是最小化方向，但在实际操作中，这是很有效的。因为SGD的速度会比批处理梯度下降快很多数量级，而且这些噪音有助于神经网络的训练。
   
4. skip-gram和negative-sampling

   最终的概率矩阵十分稀疏，因为词库中的大多数词都不和中心词同时出现，所以计算时采用从负例中采样的方法。得到的目标函数形如：
   $$
   J_t(\theta) = log\;\sigma(u_o^Tv_c)+\sum_{j\sim\;P(w)}[log\;\sigma(-u_j^Tv_c)]\\
   其中P(w) = \frac{U(w)^{\frac{3}{4}}}{Z}\\U(w)是每个词的一元分布，这里为了让更少的词更容易被选出，所以加了\frac{3}{4}\\
   由于\sigma(-x) = 1 - \sigma(x),这里达到取反例的效果
   $$

5. GloVe

   首先遍历整个语料库，对于每个窗口，计数共同出现的向量，得到整个语料库上向量的共现矩阵。然后优化以下公式：
   $$
   J(\theta) = \frac{1}{2}\sum_{i,j=1}^{W}f(P_{ij})(u_i^Tv_j-log\;P_{ij})^2
   $$
   这里$f(x)$是为了减少太频繁词出现的概率，$P_{ij}$是共现矩阵的值

   最终得到的向量是$X_{final} = u+v$

## 2. 词向量的应用--单词分类

1. 目标函数：
   $$
   W_y.x = \sum_{i=1}^dW_{yi}x_i = f_y\\
   其中的y代表不同的类别编号\\
   p(y|x) = \frac{exp(f_y)}{\sum_{c=1}^{C}exp\;f_c}
   $$
   写成交叉熵的形式：
   $$
   J(\theta) = \frac{1}{N}\sum_{i=1}^N-log(\frac{e^{f_{y_i}}}{\sum_{c=1}^{C}e^{f_c}}) + \lambda\sum_{k}\theta_k^2
   $$
   任务就是最小化交叉熵。

   重要的图：

   ![1571627506059](D:\YOGA710\Documents\1571627506059.png)




## 2. 问答系统

1. 将问题通过双向LSTM表示为向量$q$

   - 将问题编码
   - 选择LSTM两个最终的状态，连接之后作为问题的编码表示、

2. 将文档的每个词Bi-LSTM编码$\hat{p}_i$

3. 对每一个词的位置使用Attention机制
   $$
   \alpha_i\;\;=\;\;softmax(q^TW_S\hat{p}_i)
   $$
   使用两遍这样的Attention机制，就能找出答案的起止位置。

***对于一个简单的模型，如果能够将参数调整地足够好，那么就有可能击败更加复杂的模型*。**

这个模型的参数训练是端到端的，你的训练目标是是否正确地预测出了答案的起始位置和终止位置。

![1575593745474](C:\Users\YOGA710\AppData\Roaming\Typora\typora-user-images\1575593745474.png)

这些神经网络系统比传统系统性能好很多，是因为什么呢？

对于一些问题和答案语句之间语义关联特别小的例子，神经网络的优势就突显出来了。

 